#bandwidth #bottleneck #wireguard

## ‚öôÔ∏è Common Factors Impacting WireGuard Performance

### 1. **CPU Overhead & Single-Core Bottleneck**

- WireGuard encryption/decryption is handled by the CPU. On many systems, it's limited to a **single CPU core**, often capping throughput.
    
- Example: a benchmark on a laptop saw ~1.5‚ÄØGbps on a loopback test, while the host could hit 40‚ÄØGbps without WireGuard[MikroTik community forum+8Reddit+8gl-inet.com+8](https://www.reddit.com/r/WireGuard/comments/pxr6lc/wireguard_throughput_bottleneck/?utm_source=chatgpt.com).
    
- On Raspberry Pi devices, performance may be limited to around **50‚Äì150‚ÄØMbps**, depending on the model[Reddit](https://www.reddit.com/r/WireGuard/comments/pxr6lc/wireguard_throughput_bottleneck/?utm_source=chatgpt.com)[MikroTik community forum](https://forum.mikrotik.com/t/wireguard-tunnel-speed-problem/171593?utm_source=chatgpt.com)[startupdefense.io](https://www.startupdefense.io/blog/wireguard-the-next-generation-vpn-protocol?utm_source=chatgpt.com).
    

### 2. **Latency & TCP Window Effects**

- While WireGuard uses UDP, most of the data you send (e.g. Netflix) is TCP. High latency along with TCP window size limits affect throughput.
    
- The TCP throughput formula:

```
Window size = Throughput √ó RTT
```

- For connections with **~200‚Äì300‚ÄØms latency**, sustained throughput may be capped around **50‚Äì200‚ÄØMbps**, even if raw network bandwidth is higher[gl-inet.com](https://www.gl-inet.com/blog/why-wireguard-vpn-speeds-drop-with-high-latency/?utm_source=chatgpt.com).
    

### 3. **Protocol and MTU Overhead**

- WireGuard encapsulates traffic with approximately **48 bytes of extra overhead** (IP + UDP headers + nonce and authentication tags).
    
- This reduces the effective MTU and slightly lowers maximum throughput unless you manually adjust MTU settings[researchgate.net+12Wikipedia+12blog.howardjohn.info+12](https://en.wikipedia.org/wiki/WireGuard?utm_source=chatgpt.com).
    

### 4. **Implementation Variants: WireGuard-Go vs Kernel**

- **Kernel implementation (WireGuard-C)** is significantly faster and more efficient.
    
- **WireGuard-Go** (user-space) consumes more CPU (sometimes saturating a single core) and yields lower throughput and higher latency compared to kernel mode[rp.os3.nl](https://rp.os3.nl/2019-2020/p71/report.pdf?utm_source=chatgpt.com).

## üìä Benchmark Results Summary

### From academic performance comparisons:

|VPN Method|TCP Goodput (through WireGuard over 1‚ÄØGb link)|Latency Overhead|
|---|---|---|
|WireGuard‚ÄëC|~901‚ÄØMbps|~0.42‚ÄØms median|
|strongSwan (IPsec)|~906‚ÄØMbps (AES‚ÄëGCM)|~0.22‚ÄØms median|
|OpenVPN (AES‚ÄëCBC)|Significantly lower (~200‚ÄØMbps)|~0.40‚ÄØms‚Äì0.73‚ÄØms[Reddit+7rp.os3.nl+7](https://rp.os3.nl/2019-2020/p71/report.pdf?utm_source=chatgpt.com)|

### From real-world user tests:

- In one 10‚ÄØGbps test using MTU tuning, WireGuard reached **~5.1‚ÄØGbps**, while OpenVPN managed only about **230‚ÄØMbps**[Reddit](https://www.reddit.com/r/linux/comments/9bnowo/wireguard_benchmark_between_two_servers_with_10/?utm_source=chatgpt.com).
    
- CPU was the limiting factor even on fast interfaces; throughput increases significantly if encryption/decryption runs multi-threaded or optimized[Reddit](https://www.reddit.com/r/WireGuard/comments/pxr6lc/wireguard_throughput_bottleneck/?utm_source=chatgpt.com).

## ‚ö†Ô∏è What This Means for Your Use Case

If someone streams Netflix via your WireGuard gateway:

- You'll consume **download bandwidth** equal to the stream bitrate (e.g. ~5‚ÄØMbps for 1080p, ~15‚Äì25‚ÄØMbps for 4K).
    
- **Real-world throughput** is often at **80‚Äì90%** of the no-VPN baseline on well-provisioned hardware; slower on low-powered or single-core routers[procustodibus.com](https://www.procustodibus.com/blog/2022/12/wireguard-performance-tuning/?utm_source=chatgpt.com).
    
- **Latency impact** is usually minor (0.2‚Äì0.5‚ÄØms), but _TCP congestion and window scaling_ across high-latency links may limit speed[gl-inet.com](https://www.gl-inet.com/blog/why-wireguard-vpn-speeds-drop-with-high-latency/?utm_source=chatgpt.com).
    

---

## üí° Recommendations to Minimize Bottlenecks

1. Use **kernel-based WireGuard (not WireGuard-Go)** on Linux.
    
2. Choose hardware with at least **dual-core or quad-core CPU support** and AES-NI/crypto acceleration.
    
3. Tune **MTU settings** (e.g. set WG interface to 1420 for IPv4) to avoid fragmentation[Reddit](https://www.reddit.com/r/WireGuard/comments/pxr6lc/wireguard_throughput_bottleneck/?utm_source=chatgpt.com).
    
4. Use **iperf3** to measure performance with and without the VPN to identify overhead:

```
iperf3 --server
iperf3 --client <other-end>
```

5. Monitor **CPU utilization** and consider adjusting `/sys/class/net/wg*/queues/...rps_cpus` to distribute cryptographic work across cores[procustodibus.com+4Reddit+4wiki.gbe0.com+4](https://www.reddit.com/r/WireGuard/comments/pxr6lc/wireguard_throughput_bottleneck/?utm_source=chatgpt.com).

## üìù In Summary

- **Common Bottlenecks**:
    
    - CPU encryption limits (often single-core)
        
    - Latency interacting with TCP window scaling
        
    - MTU overhead and configuration
        
- **Typical Delays**:
    
    - Latency overhead of ~0.2‚Äì0.5‚ÄØms
        
    - Throughput around **80‚Äì90%** of native line speed on modern hardware, significantly less on resource-limited devices
        
- **Best Practices**:
    
    - Use modern Linux with kernel WireGuard (C implementation)
        
    - Optimize CPU/MTU
        
    - Test with benchmarks like `iperf3`
      
      
### ‚úÖ **Benchmarking WireGuard**

To measure WireGuard performance, especially bandwidth and latency, you can use the following tools:

#### üì¶ Tools:

- **iperf3**: Measures network bandwidth between two endpoints.
    
- **ping**: Basic latency and packet loss test.
    
- **nload / bmon / iftop**: Live traffic monitors.
    
- **netperf**: Advanced benchmark tool for latency and throughput.
    

#### üß™ Example: Test WireGuard bandwidth with `iperf3`

1. On the server:

```
iperf3 -s
```

2. On the client:
```
iperf3 -c <WireGuard-peer-IP>
```

You can add `-t 60` to test over 60 seconds or `-u` for UDP tests.

---

### ‚öôÔ∏è Tuning WireGuard for Better Performance

WireGuard is already lightweight and efficient, but certain system-level settings can significantly affect performance.

#### 1. **MTU Optimization**

- **Why**: MTU (Maximum Transmission Unit) mismatches can cause fragmentation or dropped packets.
    
- **How**: Set optimal MTU for WireGuard interface:

```
ip link set dev wg0 mtu 1420
```

- _1420 is often a safe default with typical overhead, but test with `ping -M do -s SIZE` to find the max size without fragmentation._
    

#### 2. **CPU Affinity & Multithreading**

- **Why**: WireGuard uses modern cryptography (ChaCha20, Poly1305) that is CPU-bound, so multicore CPUs can help.
    
- **What to do**:
    
    - Use `taskset` or `cpuset` to pin WireGuard or VPN-heavy apps to less busy cores.
        
    - Ensure you have modern kernels (>= 5.x) that handle WireGuard modules efficiently.
        

#### 3. **Sysctl TCP Tuning**

Add to `/etc/sysctl.conf` or run with `sysctl -w`:

```
net.core.rmem_max=2500000
net.core.wmem_max=2500000
net.ipv4.tcp_rmem=4096 87380 2500000
net.ipv4.tcp_wmem=4096 65536 2500000
net.core.netdev_max_backlog=5000
```

#### 4. **Encryption Overhead & Offloading**

- WireGuard doesn‚Äôt yet support hardware offloading (unlike IPsec). On low-powered devices (Raspberry Pi, embedded), CPU becomes the bottleneck.
    
- On high-throughput gateways (e.g. 1 Gbps), expect 30‚Äì40% CPU load with 500 Mbps traffic.
    

#### 5. **Persistent Keepalive**

- **Why**: Ensures NAT traversal and quicker reconnections.
    
- **How**: Add to `[Peer]` section:
  
```
PersistentKeepalive = 25
```

### üî• Expected Delays / Bottlenecks

|Bottleneck|Description|
|---|---|
|CPU Limits|Especially on routers or low-end VPS; WireGuard is CPU-bound.|
|MTU / Fragmentation|Incorrect MTU causes packet loss or retransmits.|
|NAT / Firewall|NAT traversal delays initial connections or keepalive.|
|Buffer Bloat|Queues on routers can delay TCP responses under congestion.|
|Encryption Overhead|All traffic is encrypted ‚Äî can reduce effective bandwidth.|
|Single-threaded Routing|Some routing stacks or iptables rules aren‚Äôt parallelized well.|

### üìò Sources Used

- WireGuard Performance Docs
    
- Linode: Optimize VPN with WireGuard
    
- Tailscale MTU Optimization
    
- Real-world benchmarking by bloggers and engineers (search terms: _WireGuard iperf3 benchmark, MTU tuning, WireGuard CPU usage_)
  
  